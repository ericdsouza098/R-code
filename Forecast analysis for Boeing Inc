##View Report: file:///Users/Eric/Final_assignment_Eric.html

---
title: "Econ 4210 Term Paper"
author: "Eric D'Souza"
date: "April 7th, 2020"
output: 
 html_document: 
    toc: true 
    toc_float: 
      toc_collapsed: true
    toc_depth: 3  
    number_sections: true
    theme: lumen
fontsize: 12pt
---
```{rsetup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)}
```

# Introduction
Boeing is the world's largest aerospace company and is a leading manufacturer of commercial jetliners, defense, and  space & security systems. Boeing being one of America’s biggest manufacturing exporter, supports airlines and U.S. and allied government customers in more than 150 countries.From it's envisionment to execution, Boeing takes pride in listing itself among the largest global aerospace manufacturers. It is the second-largest defense contractor in the world based on 2018 revenue, and is the largest exporter in the United States by dollar value.

Boeing's flagship product and tailored services include commercial and military aircrafts, satellites, weapons, electronic and defense systems, launch systems, advanced information and communication systems, and performance-based logistics and training. Boeing’s commercial airplanes segment produces over 60% of sales and two-thirds of operating profit. 

The company was founded by William Boeing in Seattle, Washington on July 15, 1916. The present corporation is the result of the merger of Boeing with McDonnell Douglas on August 1, 1997. The Boeing Company has its corporate headquarters in Chicago, Illinois. The company is organized into five primary divisions: Boeing Commercial Airplanes (BCA), Boeing Defense, Space & Security (BDS), Engineering, Operations & Technology Boeing Capital and lastly Boeing Shared Services Group. In 2017, Boeing recorded US$93.3 billion in sales and ranked 24th on the Fortune magazine "Fortune 500" list. Boeing is incorporated in Delaware, USA.

In 2019, Boeing's global reputation, commercial business, and financial rating suffered and was a victim of public defamation due to mechanical erros which resulted in the  737 MAX getting declared  grounded worldwide following two fatal crashes in late 2018 and early 2019. These ineffiencies flowed into the new year of 2020, where Boeing's 737 MAX Jet was found to have debris in the fuel tanks. After deeper analysis, some parked Boeing 737 MAX jets were found to have foreign objects in their fuel tanks, even before takeoff.

This paper takes Boeing's quarterly sales data to 1) Analyze the historical pattern of Boeing's sales data and underlying factors 2) use a set of comprehensive forecasting models to fit the data and select the best-performing model to condcut further analysis with the objectiv eof obtaining better inferences 3) forecast future sales using the best performing model.

The insights generated by this report aim to assess past managerial decisions with respect to revenue performance and provide future managerial guidance to the current incumbent team using the analysis and forecasted values to better navigate the current times.

# Data description and source 
Upon research, I was able to distill that the aircraft industry has several variables that affects the overall metrics of success for any company operating in that respective space. Variables like : Income per capita, seasonal ticket prices, industrial production index, inflation and exchange rate have been accepted as the factors affecting aviation demand considering the overall affect it has on final sales.

**The selected data include in this report includes:**
1) Boeing’s quarterly sales figures (in Millions of US dollars) from Q1 2000 to Q3 2019. This data set was  directly sourced and obtained from Bloomberg Terminal.

2) Airbus's quarterly sales figures (in Millions of US dollars) from Q1 2000 to Q3 2019. This data set was  directly sourced and obtained from Capital IQ.

3) U.S. Regular Conventional Retail Gasoline Prices (Dollars per Gallon) from Q1 2000 to Q3 2019. This data set was  directly sourced and obtained from Independent Statistics and analysis provided by the official US energy information administration.

The 3 datasets then are segmented into 'training' and 'testing' based on the 80% rule. That translates into a training dataset from Q1 2000 to Q4 2015, with the remaining being testing data form Q1 2016 to Q3 2019.

# Data Analysis

## Loading required packages,importing required data set (Boeing data), and finally plotting the imported data
```{r}
library(fpp2)
library(knitr)
library(seasonal)
library(vars)
library(grid)
```


```{r}
#Loading the required data set
bo_rev <- read.csv("~/Desktop/r/BoeingData.csv")
#View(bo_rev)

#Converting the dataset into a timeseries
df = bo_rev
df = ts(df, start=2000, frequency = 4)
#View(df)
DF = df[,"SALES_REV_TURN"]
AF = df[,"Rev_AB"]
OP = df[,4]
#View(OP)

autoplot(DF) + geom_line(col="red", size=1) + 
  ggtitle("Boeing quarterly revenue (millions $)") +
  labs(y="Millions",x="Years" ) +
  theme(plot.title = element_text(size=10, face = "bold") ,
        axis.text.x = element_text(size=10, face = "bold"),
        axis.text.y = element_text(size=10, face = "bold")) 
```

There is generally an upward trend for the sales of the aircrafts. There isn’t a specific reason that this trend is attributed to, but the underlying hypothesis being since the industry is an oligopoly led buy Airbus and Boeing.In business, economies of scale, helps achieve lower costs and higher margins. 

However, the following plot has no characteristics of a cyclic pattern, and a slight seasonal pattern which is attributed to purchases of aircraft in the 1st and 3rd quarter of the mentioned years. This industry isn’t affected by strong seasonality preferences to purchase aircrafts – They are usually purchased by commercial aircraft companies and maintained to last for long durations. The graph above is clearly evident of the same. The sharp quarterly drop in 2019 is attributed to the two crashes of the 737 Max Jets mentioned in the description initially.

## Trend: Centered MA using all data (n=10)
```{r}
ma(DF, order=10)

autoplot(DF) + 
  autolayer(ma(DF,10), series="Trend") +
  xlab("Year") + ylab("$ millions") +
  ggtitle("Boeing Quarterly Sales") 
```

The overall trend of data can be identified by applying centered MA. N=10 is selected for smoothing purpose. After the smoothing process, the trend of data is easier to identify, and generally re-confimrs our previous analysis of an upward trend. It reinforces the analysis from the data plot: the quarterly sales showed significant growth in the early 2000s, then declined at around 2008 and 2012 due to the financial crisis and a number of failed flight crashes repectively.  Two factors that caused changes in direction could be the 2008 financial crisis and 2012 & 2018 mechaniacal malfuntions which triggered flight crashes and deaths.

## Seasonality: Seasonal plot and log adjusted ACF
```{r}
bo = 100*diff(log(DF))

ggseasonplot(bo, year.labels=TRUE, year.labels.left=TRUE) +
  labs(y="$ (million sales)",x="Years" ) +
  ggtitle("Seasonal plot: Boeing quarterly sales")

ggAcf(bo) +  
  ggtitle("ACF plot: Boeing quarterly sales")

ggsubseriesplot(DF) +
  ylab("$ million") +
  ggtitle("Seasonal subseries plot: Boeing quarterly sales")

```

Inference from seasonal plot: As mentioned previously – The data on the graph indicates that most of the sales for Boeing takes place in the 1st and 3rd quarter effectively. Multiple years have attributed increase in sales revenue in the 1st and 3rd quarter. Over the years, Boeing announced setbacks to some of its other commercial airline programs and temporarily reduce the production rate for the 787 Dreamliner because of soft demand as a result of the global trade environment. Some interesting numbers that stood out, were the uptick in sales of Q1 for 2000. This was mainly due to operational efficiency and improved cost reductions. Another significant data point that stood out was in 2019, which was directly related to the news of the crash of the 737 Max jets, which resulted in the death of passengers on board. This created a defamed public brand image, and resulted in fall of sales.


Inference from ACF plot:After evaluating the autocorrelation, the ACF generated is a mix of both positive and negative values, with time the values generated keep oscillating. The values generally fall within the blue lines(thresholds) which indicate a trend in data. There are no vertical lines that exceed the blue lines which indicate no significant values. There is a gradual decrease  in the ACF as the lag increases. This indicates and validates that there is a trend. 


Inference from seasonal sub-series plot: The information from the seasonal sub-series plot helps analyse the quarterly trends more effectively, and acts as a testament to the the seaosnal plot where we did not observe any particular  seaosanal factors. Additnally, while the quarterly breakdown makes it  evident, that there are spaoradic changes of peaks and trough attributed to good sales cycles, followed by mechanical failures that led to the crashes and death of several airlines from the period of 2018 and 2019. These unfortunate events, in turn caused Boeing to be in the light of corporate defamation and in-turn slowed sales cycles as there had to be a motion of constant prodcut and mechanical checks before re-starting prodcution and sales efforts.

## Lag plot
```{r}
gglagplot(DF)
```

From this lag plot, it is noticeable that overall, there does appear to be a correlation and autocorrelation of the data. Given that there does seem to be a moderately strong positive trend,with occasional drops attributed to halt in prioduction whihc resulted in the decrease of sales, in the data points.  

## Box Plot and summary
```{r}
boxplot(DF, ylab = "$ (million sales)", main = "Boxplot: Boeing Revenue (quarterly)")
summary(DF)
```

From analyzing the Boxplot and Summary Statistics, we can draw out the following conclusions:
- The highest sales in a quarter for Boeing was  $28,341 (in Millions) which is represented by the upper bound line.
- The lowest sales in a quarter for Boeing was $9,910 (in Millions) represented by the lower bound line.
- The average of sales in all quarters for Boeing from 2000 to 2019 is $17,882 (in Millions)
- The median of sales in all quarters for Boeing 2000 to 2019 is $16,668 (in Millions) represented by the Thick black line

The box plot helps Boeing to assess past perofmance and relay the information to act as a feedback loop for future projections to set realistic goals and metrics of success. For example: Boeing can rightfully assume that in the future, If activation plans and  prdcution cycles kick off successfully, they can earn $22,140 (in Millions) which is earing more than 75% of historical sales.

# Preliminary Forecasting Models
The following section contains model selection and analysis of prelimanary forecasting methods:
 1) Forecasting using Benchmark methods
 2) Forecasting using Time Series Decompositions methods
 3) Forecasting using Smoothing Methods
 4) Accuracy tests for preliminary models 
 
Determining the accuracies of each method, we enable myself and the Boeing team to assess the most accurate method of forecasting in the prmeilary models before moving into the next section of **Advanced forecasting models**.

# Forecasting using Benchmark methods
For the benchmarking method the following forecast methods to be used are mentioned below. The split of training and test data will be done in this section too. Additioanl steps are broken down:
 1) Setting the training and test data for Boeing
 2) Using the training data to forecast using  six methods of forecasting (Mean, Naïve, Seasonal Naïve, Linear Model,         Multiple Linear Model, Random Walk with Drift)
 3) Consolidating all six methods of forecasting onto one graph
 4) Measuring the accuracy of each method to determine the most appropriate method


```{r}
#splitting training and test data
training <- window(DF, start=c(2000, 1), end=c(2015, 4))
test <- window(DF, start=c(2016, 1))
h = length(test)
```

## Forecasting using Average (Mean) method
```{r}
mean.fcast <- meanf(training, h=h)

autoplot(DF) +
  autolayer(mean.fcast$mean, series="Mean Method") +
  xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Forecasting Methods into the Testing Period (Mean)") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```


```{r}
checkresiduals(mean.fcast)
```

Based of the forecasted graph, the mean forecasting method does not seem to fit the data, this is primarily because of the sporadic shifts in sales, that is attributed to the financial crisis,crashes, and mechanical failures.Hence, this may not be an accurate method to use. 

Upon deeper analysis using the checkresiduals(), the time plot seems to be volatile in nature. There is an observed upward trend, but the frequency of peaks and troughs is very high and is not an accurate represntaiton of Boeing's actual sales. The ACF plot seems to  gradually decrease wiht the increase in lag, indicating a trend. There is no seasonity factor as there are no scallop shapes present. Lastly, the histogram appears to be skewed to the left side, to more negative residuals, with anomalies around the -5000 mark.  

## Forecasting using Naive method
```{r}
naive.fcast <- naive(training, h=h)

autoplot(DF) +
  autolayer(naive.fcast$mean, series="Naive Method") +
  xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Forecasting Methods into the Testing Period (Naïve)") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```

```{r}
checkresiduals(naive.fcast)
```

Based of the forecasted graph, the naïve forecasting method is a better fit than the mean method as the forecasts do present themeselves in the original dataset's region, but still does not seem to fit the data. This is primarily because of the sporadic shifts in sales, that is attributed to the financial crisis,crashes, and mechanical failures.Hence, this may not be an accurate method to use. 

Upon deeper analysis using the checkresiduals(), the time plot seems to be volatile in nature. There is no significant trend that can be observed , but the frequency of peaks and troughs is very high towards the earlier years, but gradually decreases towards the later years, but with an increased delta of these peaks and troughs. The ACF plot seems to  gradually decrease with the increase in lag, indicating a trend.There are spikes at lag 1, 4, and 5, but become less significant after the 5th lag. There is no seasonity factor as there are no scallop shapes present. Lastly, the histogram appears to fit in more of the data into the bell shaped curve and is skewed to the right side, to more positive residuals. 

## Forecasting using Seasonal naive
```{r}
snaive.fcast <- snaive(training, h=h)

autoplot(DF) +
  autolayer(snaive.fcast$mean, series="Seasonal naive method") +
  xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Forecasting Methods into the Testing Period (Seasonal naive method)") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```

```{r}
checkresiduals(snaive.fcast)
```

Based of the forecasted graph, the seasonal naïve forecasting method is a better fit than the above two methods as the forecasts do present themeselves in the original dataset's region and are clsely fitting the direction of peaks and troughs. The data still isnt the best fit, consideirng the deltas of the drops, which dont seem to align with the dataset.  

Upon deeper analysis using the checkresiduals(), the time plot seems to be volatile in nature. There is no significant trend that can be observed , but the frequency of peaks and troughs is very high towards the middle to later years. The ACF plot seems to  gradually decrease with the increase in lag, indicating a trend.There are spikes at initial lags 1, they become insignificant in the middle, and increase significantly around lag 14 and 16. No major spikes cross the blue band, which indicates values may not be that significant.There is no seasonity factor as there are no scallop shapes present. Lastly, the histogram appears to fit in more of the data into the bell shaped curve, much better than the previous two methods, but is still slightly skewed to the right.The continous skewness towards the right may indicate undervaluation of sales.

## Forecasting using Linear model
```{r}
tslm.fit <- tslm(training ~ trend, data=training)
tslm.focast <- forecast(tslm.fit, h=h)

autoplot(DF) +
  autolayer(tslm.focast$mean, series="Linear Method") +
  xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Forecasting Methods into the Testing Period (Linear method)") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```

```{r}
checkresiduals(tslm.focast)
```

Based of the forecasted graph, the linear trend forecasting method is relatively the same in comparison to the naive method. The forecasted period does seem to lay in the region as the data set but is not able to cpature the troughs of the orginal dataset, hence making it an in-effective and in-accurate forecast . The data still isnt the best fit consideirng the constant increase, which dont seem to align with the dataset.  

Upon deeper analysis using the checkresiduals(), the time plot seems to be volatile in nature. There is no significant trend that can be observed , but the frequency of peaks and troughs is very high towards the earlier and middle  years. The ACF plot seems to  gradually decrease with the increase in lag, indicating a trend.There are multiple spikes at the initial lags which cross the blue bband incidicating a trend facotr and siginicficance of the values. There is  a slight seasonity factor as there is a  scallop shapes present between lags 2 and 4. Lastly, the histogram appears to fit in more of the data into the bell shaped curve, but is still slightly skewed to the left. Again, there seems to be a constant skew, which represents under and over valuaitons of sales. This is not ideal considering the scope of in-effective forecasts and deciosns that could be made.

## Forecasting using Multiple linear model
```{r}
Mlr.fit <- tslm(training ~ trend + season)
Mlr.focast <- forecast(Mlr.fit, h=h)

autoplot(DF) +
  autolayer(Mlr.focast$mean, series="Multiple Linear Method") +
  xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Forecasting Methods into the Testing Period (Multiple linear method)") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```

```{r}
checkresiduals(Mlr.focast)
```

Based of the forecasted graph, the multiple linear trend forecasting method is the best. The forecasted period does seem to lay in the region as the data set and is replicaitng the peaks and troughs in an identical manner.This method is one of the best ofrecasted represntation we we have seen thus far. Will need further validation, to confirm the hypothesis.  

Upon deeper analysis using the checkresiduals(), the time plot seems to be volatile in nature. There is no significant trend that can be observed , but the frequency of peaks and troughs mirrors that of the original dataset. The ACF plot seems to  gradually decrease with the increase in lag, indicating a trend.There are multiple spikes at the initial lags which cross the blue band incidicating a trend factor and siginicficance of the values. For the histogram, the data seems to be approaching a normal distribution. There is a slight skew to the left. This is not ideal considering the scope of in-effective forecasts and decisions that could be made.

## Forecasting using Random walk with drift
```{r}
rwf.fcast <- rwf(training, h=h, drift=TRUE)

autoplot(DF) +
  autolayer(rwf.fcast$mean, series="Random walk with drift Method") +
  xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Forecasting Methods into the Testing Period (RWF)") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```

```{r}
checkresiduals(rwf.fcast)
```

Based of the forecasted graph, the random walk with drift method is relatively the same in comparison to the linear method. The forecasted period does seem to lay in the region as the data set but is not able to capture the troughs of the orginal dataset, hence making it an in-effective and in-accurate forecast . The data still is not the best fit consideirng the constant increase, which dont seem to align with the dataset.  

Upon deeper analysis using the checkresiduals(), the time plot seems to be volatile in nature. There is no significant trend that can be observed , but the frequency of peaks and troughs is very high towards the earlier and middle years. The ACF plot seems to have a slight seasonality trend between lags 11 and 16.There are a few spikes at the initial lags which cross the blue band, but not frequent enough to have the value be considered as significant.Lastly, the histogram appears to fit in more of the data into the bell shaped curve, but is still slightly skewed to the right Again, there seems to be a constant skew, which represents under valuaitons of sales. This is not ideal considering the scope of in-effective forecasts and decisions that could be made.

## Plotting all six forecasts on one graph for better comparative visualization
```{r}
autoplot(DF) +
  autolayer(mean.fcast$mean, series="Mean Method") +
  autolayer(naive.fcast$mean, series="Naive Method") +
  autolayer(snaive.fcast$mean, series="Seasonal Naive Method") +
  autolayer(tslm.focast$mean, series="Linear Method") +
  autolayer(Mlr.focast$mean, series="Multiple Linear Method") +
  autolayer(rwf.fcast$mean, series="Random Walk with Drift") +
    xlab("Years") + ylab("$ (million sales)") + 
   ggtitle( "Benchmarking Forecasting Methods into the Testing Period") +
  guides(colour=guide_legend(title="Forecasting Methods"))
```

From observing the individual graphs and now combining the graphs into one plot of forecasted data into the known environment (20% of testing data), it may be difficult to gauge the accuracy of these forecasts. 

However, when observing a graph that consolidates the 6 forecasting methods, it is clear that the Mean, Naive, random walk with drift, and linear methods are in-effective as they do not capture the direction of the orginal data set. The best method, based of the forecasted graph and checkresdial() functions was the multiple linear method. 

I will now run an accuracy test to find the best suiting method before moving forward.

## Measuring the accuracy of each method to determine the most appropriate method
```{r}
### Measuring Accuracy of Forecasts
a1 = accuracy(mean.fcast, test)
a2 = accuracy(naive.fcast, test)
a3 = accuracy(snaive.fcast, test)
a4 = accuracy(tslm.focast, test)
a5 = accuracy(Mlr.focast, test)
a6 = accuracy(rwf.fcast, test)

### Tabulating Forecast Summary Statsistics
a.table<-rbind(a1, a2, a3, a4, a5, a6)
row.names(a.table)<-c('Mean Training','Mean Test', 'Naive Training', 'Naive Test', 'Seasonal Naive Training', 'Seasonal Naive Test', 'Linear Model Training', 'Linear Model Test', 'Multiple Linear Model Training', 'Multiple Linear Model Test', 'Random Walk with Drift Training', 'Random Walk with Drift Test')

# Order the table according to MASE
a.table<-as.data.frame(a.table)
a.table<-a.table[order(a.table$MASE),]
kable(a.table, caption="Measures of Accuracy for Forecasting Methods", digits = 2)
```

Based of all the analysis, my asumtion and hypothesis was right. **The Multiple linear method** was the best representaion of the data set and also has the lowest MASE value of 1.26. It also had the lowest values in MAPE, MAE, and RMSE. 

# Forecasting using time series decomposition
For the time series decomposition method the the following forecast methods and steps are to be used and followed:
 1) Using the training data to forecast using three methods of forecasting (Classical decomposition, X11 decomposition, STL decomposition)
 2) Measuring the accuracy of each method(**in this case just for STL**) to determine the most appropriate method

## Classical decomposition
```{r}
DF %>% decompose(type="multiplicative") %>%
  autoplot() + xlab("Years")
  ggtitle("Classical decomposition of Boeing sales")
```

The following were the results of the classical decomposition method. 1) The data seems to have been over-smoothed and not captured the data accurately between the years 2002 and 2005, but the classical method has been known to over-smooth rapid rises and falls in the data, and hence is not used in forecast methods. 2) Seasonality component never changes.This method assumes that the seasonal component repeats from year to year. This is a completely invalid assumption that is made, especially consideirng he volatile and ever sporadic indistry of avaition and effects of external attrbiutes like oil prices.The classical decomposition methods are unable to capture these seasonal changes over time, hence this is a false representation.3)The trend-cycle estimate tends to over-smooth rapid rises and falls in the data  4) Significant remainders concentrate in year 2008 and 2018-2019 when economic crisis and significant malfuntions leading to crashes took place respectively.

## X11 decomposition
```{r}
DF %>% seas(x11="") %>%
autoplot() +
  ggtitle("X11 decomposition of Boeing sales")
```

The X11 decomposition method is based on classical decomposition, hence there is a sharp simillarity to the data graph of the classical decomposition method. The main differnces arise particlarly at the trend and seasonal graphs. Compared to the classical method, X11 better captures the drop  and other spikes in sales, such as the increase in 2012.

The X11 decomposition method overcomes several disadvantages that the classical decomposition inherits. Specifically, trend-cyle estimates are available for all end points and the seasonal component is allowed to vary slowly over time. The process tends to be highly robust to outliers. As such, the X11 decomposition method better captures factors in the environment that may influence the seasonality in sales data and thus, provides more accurate seasonally adjusted data for Boeing. The seasonal graph, seems to show a fluctation whihc is a better representaiton to the avaition environment and more specifically to Boeing. As such, the X11 decomposition method is preferred over the classical method when forecasting.

## STL decomposition method
```{r}
DF %>%
  stl(s.window="periodic", robust=TRUE) %>%
  autoplot()
```

The STL decomposition not only captures large spikes in the data,but also other spikes in the sales data, thus, producing more accurate seasonally adjusted data relative to the prior two methods. The only disadvantage with the STL method is that it does not process trading day or calendar, however, this limitation does not affect our analysis of Boeing’s sales data by quarter. The trend grpaah captures the original dataset accurately and better represents the data relative to the previous two methods.

As the STL is a versatile and robust method for decomposing time series , this method is preferred over the previous two methods and will be used to forecast time series decomposition.

```{r}
stl.focast <- stlf(training, method='naive', h=h)
autoplot(stl.focast, series = "STL") +
  autolayer(DF, series = "Original data")

```

Comparing the forecasted data to the testing set, although there appears to be a significant confidence interval, the overall direction of the forecasted data seems to closely follow the testing set data. But, does not accurately align with the deltas of the peak and troughs. 

To proceed, and mitgiate concerns we must further investigate the accuracy measures.

## Measuring Accuracy of Forecasts for Seasonal and Trend decomposition using Loess method
```{r}
a7 = accuracy(stl.focast, test)

a.table<-rbind(a7)
row.names(a.table)<-c('STL Training','STL Test')

# Order the table according to MASE
a.table<-as.data.frame(a.table)
a.table<-a.table[order(a.table$MASE),]
kable(a.table, caption="Measures of accuracy for time series decomposition forecasting methods", digits = 2)
```

As the STL mehtod was the only forecast computed under the time series decompositon methods, the MAPE is 8.88 and MASE of	1.28. This method has already been eliminated considering the multiple linear method values being lower.

# Forecasting using Simple exponential smoothing methods
For the Simple exponential smoothing methods the following forecast methods and steps are to be used and followed:
 1) Using the training data to forecast using six methods of forecasting (Simple exponential smoothing, Holt's method, Holt's method with      damping, Holt-winter's seasonal method - additive, Holt-winter's seasonal method -multiplicative, and finally ETS method)  
 2) Measuring the accuracy of each method to determine the most appropriate method
 
## Forecasting using simple exponential smoothing
```{r}
ses.fcast <- ses(training, h=h)

autoplot(ses.fcast, PI=FALSE) +
  autolayer(fitted(ses.fcast), series="SES") +
  autolayer(DF, series="Original Data") +
  ylab("Boeing sales ( $ million)") + xlab("Year")
```

The simple exponential smoothing method is suitable for forecasting data with no clear trend or seasonal pattern. It failed to identify and forecast the recent changes in direction in sales. It was in line with the data (training) , but flattened out with a slight inclination pointing towards a downwards trend. 

As established in the Data Analysis section, the original dataset of Boeing does appear to have a clear trend pattern and thus, this method may not be the best method to forecast.

## Forecasting using Holt's method
```{r}
holt.fcast <- holt(training, h=h)

autoplot(DF) +
  autolayer(holt.fcast$mean, series="Holt's method") +
  ggtitle("Forecasts from Holt's method with damping") +
  xlab("Year") + ylab("Boeing sales ( $ million)") +
  guides(colour=guide_legend(title="Forecast"))
```

Holt’s method appears to forecast sales to rapidly increase Clearly, in comparison to the testing set, this method seems to be inaccurate and not a representation of the original data. This is supported by  evidence that indicate that this methods tends to over-forecast, especially for longer forecast horizons which in our case is true. 

## Forecasting using Holt's method with damping
```{r}
holtd.fcast <- holt(training, damped=TRUE, phi = 0.9, h=h)

autoplot(DF) +
  autolayer(holtd.fcast$mean, series=" Holt's method with damping") +
  ggtitle("Forecasts from Holt's method with damping") +
  xlab("Year") + ylab("Boeing sales ( $ million)") +
  guides(colour=guide_legend(title="Forecast"))
```

In line with the previous analysis, Both Holt’s method and Damped Holt’s method appear to forecast sales to rapidly increase. The expalanation and analysis remains the same. The evendince mentioned in the previous Holt method analysis This is supported by empirical evidence that indicate that these two methods tend to over-forecast for longer forecast horizons. Both these methods are not accurate and should not be used for forecasting as Boeing could suffer tremendous failure throughr realizing or setting the right objectives.

## Forecasting using Holt-winter's seasonal method -multiplicative
```{r}
hw.fcast.mu <- hw(training,seasonal="multiplicative", h=h)

autoplot(DF) +
  autolayer(hw.fcast.mu$mean, series="HW multiplicative forecasts") +
  xlab("Year") + ylab("Boeing sales ($ million)") +
  guides(colour=guide_legend(title="Forecast"))
```

The forecast plot for the Holt-winter's seasonal method - (multiplicative)  appear to follow the general trend of the testing set, however, there does appear to be a signiifcant over-estimates in the testing data set. This is this the case as this method is generally used to forecasting data with a seasonality factor; as mentioned thruoghout the analysis, seasonality factor does not seem to be prominent in Boeing’s sales data. Hence making this method not the best for our forecasting needs.

## Forecasting using ETS method
```{r}
training %>% forecast(h=h) %>%
  autoplot(PI=FALSE) +
  autolayer(DF, series="Original Data") +
  ylab("Boeing sales ($ million)")
```

Clearly, based over the visual reprentation of the graph the ETS method does appear to produce over-estimates while still following the trend with an inclation to dipat Q3 of 2019. However, further investigations of accuracy measures must be conducted.

For a large proportion of the forecasts of the smoothing methods there seems to be an overall increasing trend for the forecasted period while the actual sales data does not represent the same. This discrepancy  may be caused by the nature of the exponential smoothing approach which genrally places  an increased weight for data of that has already occured, hence not really being able to account for the sporadic changes that took place in 2018 and 2019.

We will now conduct a accuracy test to re-confirm our analysis.

## Measuring Accuracy of Forecasts for smoothing methods
```{r}
### Measuring Accuracy of Forecasts
a8 = accuracy(ses.fcast, test)
a9 = accuracy(holt.fcast, test)
a10 = accuracy(holtd.fcast, test)
a12 = accuracy(hw.fcast.mu, test)
a13 = accuracy(training %>% forecast(h=h), test)

### Tabulating Forecast Summary Statsistics
a.table<-rbind(a8, a9, a10,a12, a13)
row.names(a.table)<-c('SES Training','SES Test', 'Holt Training', 'Holt Test', 'Holt Damped Training', 'Holt Damped Test','HW Multiplicative Training', 'HW Multiplicative Test', 'ETS Training', 'ETS Test')

# Order the table according to MASE
a.table<-as.data.frame(a.table)
a.table<-a.table[order(a.table$MASE),]
kable(a.table, caption="Measures of Accuracy for Simple exponential smoothing forecasting methods", digits = 2)
```

Running through all of the mentioned tests, the SES method of forecasting appears to be the most accurate with lowest values of MPE = -6.04, MAPE = 9.25, MASE = 1.29. 

# Forecasting using Advanced models
For  forecasting using advanced models we are going to use the following few forecast methods and follow each repective step:
 1) Using the training data to forecast using six methods of forecasting (ARIMA, Vector auto regression(VAR), TBATS and NNETAR)  
 2) Measuring the accuracy of each method to determine the most appropriate method
 
## Forecasting using ARIMA method
```{r}
arima <- auto.arima(training)
arima.fcast <- forecast(arima, h=h)

autoplot(arima.fcast) +
  autolayer(DF, series="orignal data") +
  ylab("Boeing sales ($ million)") + ggtitle("Forecasting using ARIMA")

```
Looking at the the forecasted output it is evident that the ARIMA method appears to over-estimate forecasts for the test period . As such, this may not be the most accurate method to forecast into the future, as this could lead to in-accurate and in-effective decison making for the team at Boeing, which could result in a costly mistake.

## Forecasting using Vector auto regression(VAR) method
```{r}
#The column numbers correpsond to the following columns: 4=Gasoline prices  | 3=Airbus revenue | 2=Boeing revenue
var_data = log (df[,c(4,3,2)])
plot(var_data, main = "Vector-Autoregression Data", xlab = "Years")
```

Before moving forward, I want to clarify what each of these columns stand for to make analysis clear. 

**1) US_OP = U.S. Regular Conventional Retail Gasoline Prices (Dollars per Gallon)**
**2) Rev_AB = Airbus's revenue**
**3) SALES_REV_TURN = Boeing's revenue**

The Vector Autoregression Model (VAR) analyzes the dynamic interaction among variables. I have decided to pursure with U.S Gasoline prices and Airbus's revenue prjections because 1) Oil/Gasoline/Jet fuel prices was leading factor that helped detemrine supply and demand of aircrafts  and was one of the mostly adopted economic indicators and its measurement has little dicrepancy. 2) Competitor pricing models is always closely evaluted and monitored irrespective of the industry. For Boeing it is crucial to see how Airbus prices thier aircrafts and how they can remian as close to thier prices or be able to lower thier prices  through efficiency in production.

By converting the relevant data sets into a log series for VAR, we can better visualize the relationships between these variables. With reference to the vardata array code and best practices, I decided to order the data sets  in priority of investigation. For instance, the change in oil prices can help determine the forecast for supply and demand of aircrafts in the avaition industry, relative to revenue figures of just one company, which in our case would be Airbus.

Through the plot of the post-log-adjustment datasets, we can visualize the relationships among them. Noticably, it appears that the U.S gasoline prices and Airbus data sets don’t have a strong correlation with Boeing sales. However, we have to run further analysis to come to a definite conclusion.

**Conducting following tests to determine appropriate lag**
```{r}
VARselect(var_data, lag.max = 9, type = "both", season = 4)
```

VARselect function identifies the proper lag for a VAR model under different metrices. Type “constant + trend” is used to capture "both" level and trend, along with a lag.max = 9, which should be sufficient, covering the span of  more than 2 years’ data. Also, season = 4 is selected to match quraterly data in datasets which we have been follwoing throughout our analysis.

Based of previous learing and assignments, I understood it would be best to consider lag values suggested by having the "type=both". An explantion for the same is mentioned in the next paragraph. In order to determine the optimal number of lags, we must look at the AIC and BIC information criteria from the results. Note that BIC is another name for the Schwarz Criterion (SC). 

The VARselect() function, generated the following  results; the BIC criteria(SC) suggests a lag of 1 for the "type=both",it is usually better to use the lag quantity specified by the BIC criterion(SC). This is because a lower BIC implies either fewer explanatory variables, better fit, or both.The **BIC generally penalizes free parameters more strongly than does the AIC**

However, to confirm our hypothesis we will conduct some further tests to truly determine the optimal number of lags.


```{r}
var.1 = VAR(var_data, p = 1, type = "both", season = 4)
serial.test(var.1, lags.pt=9, type="PT.asymptotic")
```

The serial test function tests for whether there is correlation in the residual serials. A high p-value, which is generally above 0.05 is desired to prove that the residuals are not correlated. Model var.1 with p=1 passes the serial test as (0.4286>0.05). Hence, we dont need to test for a model var.2 with p=2 or greater.Thus, the residuals are uncorrelated as we wanted and we proceed with var.1.

```{r}
roots(var.1)
```

The roots of various VAR models were calculated in an attempt to identify the best fitting VAR model in tandem with the AIC and BIC analysis that we did above. We did this to try and make a bullet proof decsion of moving ahead with a lag of 1. As we beleive that the lower BIC is a better idnivaiton of a better model.

A good-fitting VAR model should have the root values of the characteristics polynomial under 1. From the results, we can see that var.1 has  accomplished polynomial values below 1 which account to 0.9339831 respectively. Thus, this indicates that these VAR models are good fits and proceed with the model. 

**Analyzing residuals**
```{r}
acf(residuals(var.1), type="partial", lag.max=10)
```

From the various plots of residuals shown above, it appears that the residuals are random in nature this is evident from the residual plots and the ACF plots generated. The ACF particularly dont show any common trend or seasonality traits and have most of the spikes within the confidence bands, which are indicated by the blue dotted lines. Thus, this indicates that the VAR model with a lag of 1 (BIC-suggested) is a good fit for our model.

**Granger Causality**
```{r}
causality(var.1, cause = c("Rev_AB", "US_OP"))
```

Granger Causality refers to whether or not one variable is useful in forecasting the values of another variable. Put differently, Granger Causality examines if the movements of one variable preceeds or infleunces the movement of another variable in the data set. Granger Causality tests can be executed through the use of standard test statistics on the estimated coefficients.

In our case specifically, since the focus is currently on seeing the effects of external variables on Boeing's revenue, the external variables here would be the U.S.gasoline prices and Airbus's revenue.If causality is present, null hypotheses would be rejected.

The analysis through the Granger Causality null hypothesis states that U.S. Gasoline prices and  Airbus's revenue **DO NOT** Granger-cause Boeing’s sales. The p-value genrated is 0.5202, which is greater than 0.05. Thus, we will not reject the null hypothesis Which results to a conclusion that **U.S. Gasoline prices and  Airbus's revenue do not affect Boeing's sales**. Unfortunately, Boeing's management team cannot use these 2 data sets to forecast Boeing sales to provide more information when preparing management goals and expectations.The alternative would reside in fidning more specifc datasets like maybe end prodcut proces like Fuel jet prices etc.

Secondly, the instantaneous null hypothesis states that there is NO instantaneous causality between U.S. Gasoline prices, Airbus's revenue and Boeing's sales. It is evident that the p-value of 0.758 is higher than 0.05. This means that we DO NOT REJECT the null.

**Impulse Response Function**
```{r}
#Sales_Rev_Turn = Boeings revenue
var1.imrf <- irf(var.1, response = "SALES_REV_TURN",  n.ahead = 16, boot = TRUE,  runs=500, seed=99, cumulative=FALSE)

par(mfrow=c(1,1))
plot(var1.imrf, plot.type = "single")
```

Impulse Response Functions (IRFs) are used to help us visualize how a shock to one variable can affect the response of other variables. A shock refers to temporary but extremely sudden change to one variable. In our case we want to unocver and sicover how a shock to U.S gasoline prices and Airbus's sales impacts Boeing's sales. Further explanation will help unocver the overall effect of this analysis.

From the IRF’s, it becomes evident that a shock to U.S. gasoline proces  will cause a gradual increase in Boeing's sales, which reaches a peak by the fourth quarter, this increase eventually flattens out to a value higher than the initial value. On the other hand, a shock to Airbus's sales causes an immediate and sharp increase in the first quarter of Boeing's sales which immediately drops and flattens out for the rest of the quarters moving forward, when no decrease or trough in the future. 

Lastly, as the two red lines for both graphs do not go above the grey line (representing 0), we can coclude that both U.S.gasoline prices and Airbus revenue do not have a statisitically significant impact on Boeing's sales.

**Forecast Error Variance Decomposition**
```{r}
fevd(var.1, n.ahead = 16)
```

The Forecast Error Variance Decomposition (FEVD) is a method of analyzing the dynamic interaction that occurs between variables. The FEVD displays how much of one variable’s s-step ahead forecast error variance can be attributed to the other variables. It general terms it presents the relationship of changes regarding the three variables. Let’s take a closer look at the analysis and the assosiated FEVD results.

Through my results, it becomes evident that U.S gasoline prices explains a insignificant  amount of the forecast error variances of Boeing's and Airbus's revenue (ranging from 0 to 0.125% across the 16 quarter period), this is in line with our intial inference that we gathered about the  varibales have no effect on Boeing's revenue.

The revenue of Airbus, explains a considerable amount of the forecast error variances of U.S. gasoline prices which ranges from 1.1 to 11.4% acorss the 16 quarter period. But, it does not have that much of a considerbale effect on the sales of Boeing as the forecast error variances ranges from 3.4 to 4.8% over the same 16 quarter period.

Finally, Boeing's sales explains an insiginficant amount to both U.S. gasoline prices and Airbus's sales ranging from 0.5 to 0.6% and 1 to 1.5% respectively.

Therefore, The FEVD test shows that the three variables are inter-related to a certain extent over the 16 quarter period.

<font size="5">**Training, Testing and forecasting overall data**</font> 

```{r}
trainingdata <- window(df[,c(4,3,2)], start=c(2000,1), end=c(2015,4))
testdata <- window(df[,c(4,3,2)], start=c(2016,1))

# We choose to move forward with the model of lag 1, suggested by  BIC(SC)
var2 = VAR(testdata, p=1, type = "both", season =4)

```

```{r}
var.fc = predict(var.1, n.ahead= h)
plot(var.fc)
```

Interestingly, the confidence band for Airbus's sales is the tightest and Boeing’s sales the acceptable. The visualization of forecasting results comply with previous findings: The three avriables have very less in common.Visually, all three forecasts seem to be acceptable fits as continuation of actual data.

```{r}
var.fc1 = forecast(var2, h= h)
autoplot(var.fc1) + xlab("Year")
```

```{r}
var.fc1$forecast$SALES_REV_TURN
exp (var.fc1$forecast$SALES_REV_TURN$mean)
```

## Forecasting using TBATS method
```{r}
tbats = tbats(training)
tbats.fcast <- forecast(tbats, h=h)
#autoplot(DF) +
  #autolayer(tbats.fcast, series="tbats") +
  #ylab("Boeing sales ($ million)")
autoplot(tbats.fcast, series="TBATS") +
  autolayer(DF, series="Original Data") +
  ylab("Boeing Sales $(millions)")
```

The TBATS method seems to also underestimate forecasted sales and does not account for the changes and shifts in the sales of Boeing. Similar to the ARIMA method, this may not be the most accurate method.TBATS do not allow for exogenous variables to be added to the model to improve forecasts. 

## Forecasting using NNETAR method
```{r}
fnnetar = nnetar(training)
nnetar.fcast <- forecast(fnnetar, h=h)
autoplot(DF) +
  autolayer(nnetar.fcast, series="nnetar") +
  ylab("Boeing sales ($ million)")
```

The NNETAR method has done a decent job is capturing the dynamic nature of the sales data of Boeing. The values are definetely over-estimated for the test period, with  delta changes from peak to trough underestimated. From a visual observation, this might be a competative model, but will need further research.

## Conducting accuracy tests
```{r}
a14 = accuracy(arima.fcast, test)
a16 = accuracy(tbats.fcast, test)
a17 = accuracy(nnetar.fcast, test)

### Tabulating Forecast Summary Statsistics
a.table<-rbind(a14,a16, a17)
row.names(a.table)<-c('ARIMA Training','ARIMA Test','TBATS Training', 'TBATS Test', 'NNETAR Training', 'NNETAR Test')

# Order the table according to MASE
a.table<-as.data.frame(a.table)
a.table<-a.table[order(a.table$MASE),]
kable(a.table, caption="Measures of Accuracy for Advanced Forecasting Methods", digits = 2)
```

```{r}
#a15 = accuracy(var.fc2, test)
#a15
```

## Comparison of best four models

**Benchmark Methods**:  The multiple linear method had the smallest Mean Absolute Scaled Error (MASE) of 1.26.

**Time Series Decomposition Methods**: The  STL decomposition method had the lowest MASE of 1.28.

**Smoothing Methods**: The SES method had the lowest MASE of 1.29.

**Advanced forecasting methods**: The TBATS method had the lowest MASE of 9.8

As the multiple linear method had the lowest MASE of 1.26 of all methods, this is the most accurate method to forecast into known and unkown environments.

## Forecasting using the  multiple linear method model 
```{r}
autoplot(DF, series = "Actual data") + 
  ggtitle("Boeing quarterly sales") +
  autolayer(Mlr.focast$mean, series="Multiple Linear Method", PI=FALSE) + 
  ylab("Sales")
```

```{r}
### Number of Quarters to Forecast into Unkown Environment
h2 = 6

### Forecast into next 6 Quarters with Mean Method
#tslm.plot <- tslm(DF, h=h2)
#autoplot(mlr.plot) + 
 # autolayer(DF, series="Original Data") 

fcast_mlr_alldata <- tslm(DF ~ trend + season)
Mlr.focast <- forecast(fcast_mlr_alldata, h=h2)

autoplot(Mlr.focast) + 
  autolayer(DF, series="Original Data") 
```

If Boeing were to use the average method to forecast the next 6 quarters, the forecasted data would be $27 MM approximately at the peak for the  and then drops to $25MM in the next 6 quarters. This indicates that for the next 6 quarters, sales will be greater than they currently are. Boeing can use this insight to properly plan strategic decisions. They can properly plan production and manage mechancial fixes while invesitng in startups like they have already begun doing thourgh horizontal spaces.  

The sporadic turn of sales and public defamaiton while take time to recover, however Boeing can use extra capital earned to build up the reputaiton in other indistirres like space exploraiton and becime a fore-runner or lead collaborator in that niche area. Through this anlaysis insights from this forecast effectively helps direct Boeing’s strategic priorities.

# Conclusion 
Following thorough background research, we’ve been able to identify potential causes for the sporadic decline in Boeing’s global sales throughout the period of 2000 to 2019. Potential drivers are competitive pressures of macroeconomic factors like rising prices of fuel, customer senitment, and competative forces as the supplying of aircrafts in the aviation industry is primarily ruled as an oligopoly. 

The sporadic declines aorund the years 2008,2012, 2018 and 2019 were mainly due to the financial crisis that made th process of buying fuel more expensive and hence lowered the overall demand for planes in the avaition as peple were flying out less too. the next few years, boeing expeirenced extreme public defamation as the company had several crashes that resulted in the death of passenegrs on-board. The root cause for the same was in-efficent prodcution tests that led to mechnanical failures.

From data analysis, we have been able to identify a strong linear trend pattern, but a relatively weak seasonal pattern in the dataset. This information will guide the forecasting methods applied. In addition, we have been able to identify years of high and low volatility and used background research to best explain these volatile data points, such as the 2008 financial crisis and the system errors mentioned above.

From comparing a multitude of different forecasting models, we have been able to determine that the Mean Test is the most accurate model with a MASE of 0.73. Moving forward, we can use this model to forecast into the future to provide actionable insights for Boeing’s  team to set realistic sales and production targets and deploy necessary capital to prepare for the estiamted sales.

My last suggestion will also be to look inot horzontal indutries and the indutry of space travel as the space is now going through a mssive change of innovation, due to the fact of the whole space systme now being democratized and open to more startups. Boeing recently made an invesmtent in a space sattelite serve startup which acts as a testament to thier goal of entering new segemnts to reduce volatality.

# Refernces
1) https://www.eia.gov/petroleum/data.php
2) https://www.capitaliq.com/CIQDotNet/Financial/IncomeStatement.aspx?CompanyId=561001&statekey=e345df2e5812418eb6df189da6628d45
3) https://www.bloomberg.com/news/articles/2020-04-06/boeing-halts-all-jet-manufacturing-with-last-787-plant-closing
4) https://techcrunch.com/2019/10/08/boeing-backs-virgin-galactic-with-strategic-20m-investment/

